# 第 8 章  MCDropout 变分推断法

1. Uncertainty
首先要明确一点，对于机器学习深度学习预测的不确定性，高的不确定性对应低的可信度，可信度也叫confidence，以分类为例介绍预测过程的不确定的性，就以二分类猫狗识别为例，分类模型对某个图片的预测label是0.8，那不可以说这个预测结果有0.8的可信度让我们相信他属于label==1的类别，只能说这个模型认为是label==1的类别的可能性大。

用一个参数来量化预测的不确定性：varience，比如用baggging思想做集成模型，多个模型的预测结果的方差越大，代表这个点预测的不确定性越大，也就是uncertainty越大，因此也可以想到，最大似然估计的不确定性，是给了训练数据D，还有某条测试数据而言的，也就是训练数据与测试数据的某种关系。

所以就可以说：对于常见的最大似然估计MLE，单个的模型没有不确定性可言，只有给定数据集，还有要测试的样本，根据不同模型的预测结果的方差来评估这个不确定性。

（PS：要是学过数值计算方法，这里大家可能就能想到对于插值任务，外推的误差大于内插的误差）

2.从实际效果看dropout的效果
作者的博客里制作了一个回归任务的api，在这个简单的api里，可以点击new_data，如图1所示黑色点所处坐标就是训练数据，一个两层的神经网络训练过程中，模型通过dropout产生对数轴上每个点的预测结果，并且根据这些结果计算出每个点的均值与方差，蓝色的线代表均值，黑色线代表当前预测值，蓝色范围表示过去预测的分布从中可以看出方差，从图1可以看到，在模型收敛之后，可以看到在训练数据的上下界范围内预测的variance比较小，而在两边的方差比较大，也就是靠近训练数据的地方预测的不确定性小，而在远离训练数据的点估计预测的不确定性大。

总结：dropout使得模型对于每个点的预测值逐渐收敛至他的期望


fig.1 the mean and variance in the dropout when two layers&amp;amp;amp;#39;s neural net
blog的中间位置处有demo：

http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html
​
mlg.eng.cam.ac.uk
3.证明：Dropout为啥能表示不确定性
这里由于本人的水平有限，无法想通作者为什么可以想到这样的方式证明，所以也就无法把这项工作冰山下的宝藏展示出来，比较遗憾，不过抱着自己学习的态度也记录一下作者的做法。首先介绍高斯过程与变分法两个知识点。

3.1 高斯过程

在这里简单理解高斯过程，回归任务里理论上有无数多模型可以拟合数据，但是这些模型出现的概率不同，其中有的模型的可能性大，有的模型的可能性小，高斯过程就是针对数据生成模型的分布。如图2所示，上面的图里红色线代表最有可能的模型，淡红色阴影区域从左到右表示预测方差的分布。


fig.2 Gauss Process
学习链接：

Introduction to Gaussian Processes - Part I
​
bridg.land
图标


3.2 证明

证明过程分为三步：1.从高斯过程（Gauss Process）角度考虑得到一个目标函数；2.模型的后验概率难以计算，使用变分法计算后验概率；3.证明1，2得到的结果与deopout得到的结果是相似的。证明过程分为三步：1.从高斯过程（Gauss Process）角度考虑得到一个目标函数；2.模型的后验概率难以计算，使用变分法计算后验概率；3.证明1，2得到的结果与deopout得到的结果是相似的。

Step_1 高斯过程：


fig.3 构建高斯过程
如图，在step_1里，首先给定训练数据，当然这个简单的回归任务里，x是1d数轴上的点，y是1d的实数，定义随机模型参数w，模型参数的先验p（w），最大似然p（Y|w，X），使用变分法计算后验需要用贝叶斯公式，即后验等于先验乘以似然，使用积分的方式计算输入x*的预测值y*的分布。由于后验概率是一个反问题，这比较难算，所以接下来采用变分法的方式计算后延概率。

Step_2 变分法计算后验概率：


fig.4 minimize w的最大后验概率分布与q_theta（w）间的KL距离
直接计算最大后验概率分布是一个反问题不好算，就近似一个分布q_theta（w），使得这个分布与后验间的KL距离最小，将KL距离展开以后，再使用evidence lower bound ，即ELBD，来优化转开项的前两项，问题是等价的，详细可参考：

川陀学者：变分推断——深度学习第十九章
​
zhuanlan.zhihu.com
图标
所以问题转为计划theta上的积分，看minimize的目标函数，theta*一方面使得近似的分布与w的先验概率分布间距的KL距离小一些，另一方面使得似然是最大的。这样经过高斯过程生成的模型参数w知道以后，再用w的近似分布来对w积分就可以用来计算x*预测值的分布。

Step_3:这种推断的方式和Drpout的联系在哪里


fig.5 dropout与高斯过程的相似性
如图5所示，i表示神经网络的层的索引，k表示i层神经网络节点的索引，theta表示节点的权重，每次使用变分法计算出w的近似分布，这个近似分布满足的特点可以看到，以p的概率取均值为0，方差为delta的高斯分布，以1-p的概率取均值为m_k，方差为delta的高斯分布，证明过程如图6所示。也就说，每次计算theta时候，theta产生的w的分布的特点是以p的概率期望归0，以1-p的概率，期望为上一次的值，也就是不变，所以dropout可以理解为贝叶斯模型的近似。


fig6. 参数theta更新后w的分布
总结
这个paper从贝叶斯模型的角度分析出模型参数更新的过程中与dropout的类似，从理论上证明了dropout的这个操作到底为啥makesense，不得不说，确实很牛。